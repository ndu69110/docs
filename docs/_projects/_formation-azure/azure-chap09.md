# Citations
- https://learn.microsoft.com/fr-fr/azure/aks/active-active-solution
- https://learn.microsoft.com/fr-fr/azure/architecture/web-apps/app-service-environment/architectures/app-service-environment-high-availability-deployment
- https://learn.microsoft.com/fr-fr/azure/frontdoor/create-front-door-terraform
- https://learn.microsoft.com/fr-fr/azure/well-architected/service-guides/azure-front-door
- https://learn.microsoft.com/fr-fr/power-pages/configure/azure-front-door
- https://learn.microsoft.com/fr-fr/azure/frontdoor/best-practices
- https://www.itpro.fr/atelier-video-embarquez-pour-azure-front-door
- https://learn.microsoft.com/fr-fr/azure/frontdoor/routing-methods
- https://www.youtube.com/watch?v=LiBNZwZ5wBs


# Tokens
- prompt_tokens: 263
- completion_tokens: 9978
- total_tokens: 10241
- search_context_size: low
- cost: {'input_tokens_cost': 0.001, 'output_tokens_cost': 0.15, 'request_cost': 0.006, 'total_cost': 0.156}


# Content
# üìò Chapitre 9 : Projet 1 - D√©ploiement Haute Disponibilit√© avec Azure Front Door

## Introduction au projet

Le d√©ploiement haute disponibilit√© constitue un √©l√©ment fondamental de l'architecture cloud moderne. Cette section pr√©sente une approche compl√®te pour mettre en ≈ìuvre une solution r√©siliente utilisant Azure Front Door comme composant central de distribution du trafic.[1]

Azure Front Door est un service d'√©quilibrage de charge de couche sept (Application Layer) qui permet de diriger le trafic utilisateur vers les backends les plus performants en fonction de crit√®res d√©finis. Dans le contexte d'une architecture haute disponibilit√© active/active, deux clusters ou plus traitent simultanement le trafic, √©liminant ainsi tout point de d√©faillance unique.[1]

### Objectifs de la formation

L'objectif principal consiste √† comprendre et impl√©menter une architecture o√π :
- Plusieurs ressources dans diff√©rentes r√©gions Azure traitent activement les requ√™tes
- Le trafic se distribue automatiquement entre les r√©gions saines
- Un basculement automatique survient en cas de d√©faillance r√©gionale
- Les m√©triques et journaux de diagnostic se centralisent pour le monitoring

### Architecture g√©n√©rale de la solution

L'architecture active/active repose sur deux ou plusieurs clusters AKS identiques, chacun d√©ploy√© dans une r√©gion Azure distincte, tous configur√©s pour h√©berger les instances compl√®tes des applications.[1] Azure Front Door agit comme point d'entr√©e unique (single entry point) en acheminant le trafic r√©seau entre les r√©gions. En situation normale, le trafic se r√©partit selon la politique de routage d√©finie. En cas d'indisponibilit√© d'une r√©gion, le gestionnaire de trafic redirige l'ensemble du flux vers les r√©gions saines restantes.

---

## Pr√©paration de l'environnement

La pr√©paration de l'environnement englobe plusieurs √©tapes critiques pour assurer le succ√®s du d√©ploiement et de la gestion de l'infrastructure.

### Pr√©requis techniques

Avant de d√©buter, l'environnement de travail doit satisfaire aux exigences suivantes :

- **Abonnement Azure actif** : Un abonnement avec des permissions suffisantes pour cr√©er des ressources dans plusieurs r√©gions
- **Azure CLI** : Installation de la derni√®re version d'Azure CLI pour g√©rer les ressources Azure en ligne de commande
- **Terraform** : Installation de Terraform version 1.0 ou sup√©rieure pour l'infrastructure-as-code[3]
- **kubectl** : L'outil de ligne de commande pour g√©rer les clusters Kubernetes
- **Editeur de code** : Visual Studio Code ou un √©quivalent pour √©diter les fichiers de configuration

### Configuration des fournisseurs Terraform

La premi√®re √©tape consiste √† initialiser les fournisseurs Terraform n√©cessaires. Cela permet √† Terraform de communiquer avec les services Azure pour cr√©er et g√©rer les ressources.[3]

```hcl
terraform {
  required_version = ">=1.0"
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~>3.0"
    }
    random = {
      source  = "hashicorp/random"
      version = "~>3.0"
    }
  }
}

provider "azurerm" {
  features {}
}
```

Ce fichier de configuration `providers.tf` d√©finit la version minimale de Terraform requise, les fournisseurs utilis√©s et leurs versions respectives. Le fournisseur Azure Resource Manager (azurerm) g√®re l'ensemble des ressources Azure, tandis que le fournisseur random g√©n√®re des valeurs al√©atoires pour assurer l'unicit√© des ressources.

### Structure des ressources Azure

L'architecture requiert la cr√©ation de plusieurs ressources Azure dans chaque r√©gion :

**Ressources primaires par r√©gion :**
- Un cluster Azure Kubernetes Service (AKS)
- Une instance Azure Application Gateway pour le routage au niveau de la couche application
- Des pools de n≈ìuds Kubernetes pour ex√©cuter les conteneurs d'application
- Une instance Log Analytics pour la centralisation des m√©triques et journaux

**Ressources partag√©es :**
- Un profil Azure Front Door au niveau global
- Une instance partag√©e Log Analytics pour l'agr√©gation des donn√©es de toutes les r√©gions
- Un groupe de ressources par r√©gion pour l'organisation logique

### Initialisation de Terraform

Une fois les fichiers de configuration pr√©par√©s, l'initialisation de Terraform t√©l√©charge les plugins des fournisseurs n√©cessaires :[3]

```bash
terraform init -upgrade
```

Le param√®tre `-upgrade` assure que les versions les plus r√©centes conformes aux contraintes de version sont t√©l√©charg√©es. Cette commande cr√©e le fichier `.terraform.lock.hcl` qui verrouille les versions des fournisseurs pour garantir la reproductibilit√© du d√©ploiement.

### Variables d'environnement et param√©trage

Pour maintenir une approche flexible et reproductible, les variables de configuration se d√©finissent dans un fichier `variables.tf` :

```hcl
variable "primary_region" {
  description = "R√©gion primaire pour le d√©ploiement"
  type        = string
  default     = "westeurope"
}

variable "secondary_region" {
  description = "R√©gion secondaire pour le d√©ploiement"
  type        = string
  default     = "northeurope"
}

variable "environment" {
  description = "Environnement de d√©ploiement"
  type        = string
  default     = "production"
}

variable "app_name" {
  description = "Nom de l'application"
  type        = string
  default     = "myapp"
}

variable "kubernetes_version" {
  description = "Version de Kubernetes pour les clusters AKS"
  type        = string
  default     = "1.28"
}
```

Ces variables permettent de personnaliser le d√©ploiement sans modifier le code principal d'infrastructure. L'utilisation de fichiers `.tfvars` offre une s√©paration suppl√©mentaire entre la configuration et le code :

```hcl
# terraform.tfvars
primary_region      = "westeurope"
secondary_region    = "northeurope"
environment         = "production"
app_name            = "votingapp"
kubernetes_version  = "1.28"
```

### Authentification Azure

La connexion √† Azure se fait g√©n√©ralement via Azure CLI :

```bash
az login
az account set --subscription "ID-de-l-abonnement"
```

Cette authentification permet √† Terraform d'acc√©der aux ressources Azure avec les permissions appropri√©es. Pour les d√©ploiements en environnement de production ou CI/CD, on privil√©gie l'authentification par service principal.

---

## Code de l'application et d√©ploiement

### Structure de l'application et conteneurisation

L'application d√©ploy√©e dans cette architecture est g√©n√©ralement une application web moderne capable de s'ex√©cuter dans des conteneurs. Pour cette formation, on utilise une application de vote simple servie par une API backend.

#### Dockerfile de l'application

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app.py .
COPY templates/ templates/

EXPOSE 5000

HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:5000/health || exit 1

CMD ["python", "app.py"]
```

Ce Dockerfile cr√©e une image conteneur bas√©e sur Python 3.11 l√©ger. La directive HEALTHCHECK define les crit√®res de v√©rification de l'√©tat de sant√© du conteneur, utilis√©s par le syst√®me de sonde d'int√©grit√© de Kubernetes.

#### Application Flask simple

```python
from flask import Flask, render_template, request, jsonify
import os
from datetime import datetime

app = Flask(__name__)

# Configuration depuis les variables d'environnement
REGION = os.getenv('REGION', 'unknown')
INSTANCE_ID = os.getenv('INSTANCE_ID', 'default')

@app.route('/')
def index():
    return render_template('index.html', region=REGION, instance_id=INSTANCE_ID)

@app.route('/health')
def health():
    return jsonify({
        'status': 'healthy',
        'region': REGION,
        'instance_id': INSTANCE_ID,
        'timestamp': datetime.utcnow().isoformat()
    }), 200

@app.route('/api/vote', methods=['POST'])
def vote():
    vote_data = request.get_json()
    return jsonify({
        'status': 'success',
        'vote': vote_data.get('option'),
        'processed_by': INSTANCE_ID,
        'region': REGION
    }), 201

@app.route('/api/metrics')
def metrics():
    return jsonify({
        'region': REGION,
        'instance_id': INSTANCE_ID,
        'timestamp': datetime.utcnow().isoformat()
    }), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
```

Cette application expose plusieurs endpoints essentiels. L'endpoint `/health` permet au syst√®me de v√©rifier la disponibilit√© du service, tandis que `/api/vote` traite les requ√™tes m√©tier. Chaque r√©ponse inclut l'identifiant de la r√©gion pour tracer le routage du trafic.

### Configuration Kubernetes pour les clusters AKS

#### D√©ploiement de l'application dans AKS

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: voting-app
  namespace: production
  labels:
    app: voting-app
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: voting-app
  template:
    metadata:
      labels:
        app: voting-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "5000"
    spec:
      containers:
      - name: voting-app
        image: myregistry.azurecr.io/voting-app:v1.0.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 5000
          name: http
        env:
        - name: REGION
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['topology.kubernetes.io/region']
        - name: INSTANCE_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - voting-app
              topologyKey: kubernetes.io/hostname
```

Ce manifest Kubernetes configure le d√©ploiement de l'application avec plusieurs r√©plicas pour la redondance locale. Les probes de liveness et readiness assurent que seules les instances saines re√ßoivent du trafic. L'anti-affinit√© de pod favorise la distribution des r√©plicas sur diff√©rents n≈ìuds.

#### Service Kubernetes exposant l'application

```yaml
apiVersion: v1
kind: Service
metadata:
  name: voting-app-service
  namespace: production
  labels:
    app: voting-app
spec:
  type: LoadBalancer
  selector:
    app: voting-app
  ports:
  - port: 80
    targetPort: 5000
    protocol: TCP
    name: http
  sessionAffinity: None
```

Ce service expose l'application sur le port 80 (HTTP standard), routant les requ√™tes vers les pods sur le port 5000. Le service LoadBalancer Azure cr√©e un load balancer interne qui distribue le trafic entre les pods.

### D√©ploiement Terraform des clusters AKS

#### Configuration du groupe de ressources et du cluster principal

```hcl
resource "azurerm_resource_group" "primary" {
  name     = "${var.app_name}-rg-${var.primary_region}"
  location = var.primary_region

  tags = {
    environment = var.environment
    region      = var.primary_region
  }
}

resource "azurerm_kubernetes_cluster" "primary" {
  name                = "${var.app_name}-aks-${var.primary_region}"
  location            = azurerm_resource_group.primary.location
  resource_group_name = azurerm_resource_group.primary.name
  dns_prefix          = "${var.app_name}-primary"

  default_node_pool {
    name            = "default"
    node_count      = 3
    vm_size         = "Standard_D2s_v3"
    os_disk_size_gb = 50

    zones = ["1", "2", "3"]
  }

  identity {
    type = "SystemAssigned"
  }

  network_profile {
    network_plugin    = "azure"
    load_balancer_sku = "standard"
    service_cidr      = "10.0.0.0/16"
    dns_service_ip    = "10.0.0.10"
    docker_bridge_cidr = "172.17.0.1/16"
  }

  kubernetes_version = var.kubernetes_version

  tags = {
    environment = var.environment
    region      = var.primary_region
  }

  depends_on = [azurerm_resource_group.primary]
}
```

Cette configuration cr√©e un cluster AKS avec trois n≈ìuds r√©partis sur trois zones de disponibilit√© (zones 1, 2, 3) au sein de la r√©gion primaire. Cette distribution assure la r√©silience contre les d√©faillances de zone. L'identit√© syst√®me attribu√©e au cluster permet l'acc√®s s√©curis√© aux ressources Azure.

#### Configuration du cluster secondaire

```hcl
resource "azurerm_resource_group" "secondary" {
  name     = "${var.app_name}-rg-${var.secondary_region}"
  location = var.secondary_region

  tags = {
    environment = var.environment
    region      = var.secondary_region
  }
}

resource "azurerm_kubernetes_cluster" "secondary" {
  name                = "${var.app_name}-aks-${var.secondary_region}"
  location            = azurerm_resource_group.secondary.location
  resource_group_name = azurerm_resource_group.secondary.name
  dns_prefix          = "${var.app_name}-secondary"

  default_node_pool {
    name            = "default"
    node_count      = 3
    vm_size         = "Standard_D2s_v3"
    os_disk_size_gb = 50

    zones = ["1", "2", "3"]
  }

  identity {
    type = "SystemAssigned"
  }

  network_profile {
    network_plugin    = "azure"
    load_balancer_sku = "standard"
    service_cidr      = "10.1.0.0/16"
    dns_service_ip    = "10.1.0.10"
    docker_bridge_cidr = "172.17.0.1/16"
  }

  kubernetes_version = var.kubernetes_version

  tags = {
    environment = var.environment
    region      = var.secondary_region
  }

  depends_on = [azurerm_resource_group.secondary]
}
```

Le cluster secondaire suit une configuration identique au cluster primaire, d√©ploy√© dans une r√©gion distincte. L'utilisation de plages CIDR diff√©rentes (10.0.0.0/16 vs 10.1.0.0/16) √©vite les conflits d'adressage entre les r√©gions.

### D√©ploiement de l'application sur les clusters

#### Script de d√©ploiement de l'application

```bash
#!/bin/bash

set -e

# Variables de configuration
PRIMARY_RG="myapp-rg-westeurope"
PRIMARY_CLUSTER="myapp-aks-westeurope"
PRIMARY_REGION="westeurope"

SECONDARY_RG="myapp-rg-northeurope"
SECONDARY_CLUSTER="myapp-aks-northeurope"
SECONDARY_REGION="northeurope"

ACR_NAME="myappregistry"
ACR_RESOURCE_GROUP="myapp-rg-westeurope"

# Fonction pour obtenir les credentials du cluster
configure_cluster() {
    local rg=$1
    local cluster=$2
    local region=$3
    
    echo "Configuration de ${cluster} dans ${region}..."
    az aks get-credentials --resource-group "${rg}" --name "${cluster}" --overwrite-existing
    
    # Cr√©er le namespace de production
    kubectl create namespace production --dry-run=client -o yaml | kubectl apply -f -
}

# Fonction pour d√©ployer l'application
deploy_application() {
    local region=$1
    local instance_name=$2
    
    echo "D√©ploiement de l'application dans ${region}..."
    
    # Appliquer les manifests Kubernetes
    kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
data:
  REGION: "${region}"
  INSTANCE_ID: "${instance_name}"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: voting-app
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: voting-app
  template:
    metadata:
      labels:
        app: voting-app
    spec:
      containers:
      - name: voting-app
        image: ${ACR_NAME}.azurecr.io/voting-app:v1.0.0
        ports:
        - containerPort: 5000
        envFrom:
        - configMapRef:
            name: app-config
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: voting-app-service
  namespace: production
spec:
  type: LoadBalancer
  selector:
    app: voting-app
  ports:
  - port: 80
    targetPort: 5000
EOF
}

# Obtention des credentials pour les deux clusters
echo "=== Configuration des clusters AKS ==="
configure_cluster "${PRIMARY_RG}" "${PRIMARY_CLUSTER}" "${PRIMARY_REGION}"
configure_cluster "${SECONDARY_RG}" "${SECONDARY_CLUSTER}" "${SECONDARY_REGION}"

# D√©ploiement du cluster primaire
kubectl config use-context "${PRIMARY_CLUSTER}"
deploy_application "${PRIMARY_REGION}" "primary-instance"

# D√©ploiement du cluster secondaire
kubectl config use-context "${SECONDARY_CLUSTER}"
deploy_application "${SECONDARY_REGION}" "secondary-instance"

echo "=== D√©ploiement termin√© ==="
kubectl config use-context "${PRIMARY_CLUSTER}"
echo "Contexte actif : ${PRIMARY_CLUSTER}"
```

Ce script automatise le processus de configuration des deux clusters et le d√©ploiement de l'application avec les configurations r√©gionales sp√©cifiques. L'utilisation de ConfigMaps permet aux pods de conna√Ætre leur r√©gion et instance d'appartenance.

### Cr√©ation du registre de conteneurs Azure

```hcl
resource "azurerm_container_registry" "acr" {
  name                = var.acr_name
  resource_group_name = azurerm_resource_group.primary.name
  location            = azurerm_resource_group.primary.location
  sku                 = "Premium"
  admin_enabled       = false

  tags = {
    environment = var.environment
  }
}

# Attribution de r√¥les pour l'acc√®s au registre
resource "azurerm_role_assignment" "aks_primary_acr" {
  scope              = azurerm_container_registry.acr.id
  role_definition_name = "AcrPush"
  principal_id       = azurerm_kubernetes_cluster.primary.identity[0].principal_id
}

resource "azurerm_role_assignment" "aks_secondary_acr" {
  scope              = azurerm_container_registry.acr.id
  role_definition_name = "AcrPush"
  principal_id       = azurerm_kubernetes_cluster.secondary.identity[0].principal_id
}
```

Le registre de conteneurs centralise le stockage des images Docker utilis√©es par les deux clusters. Les attributions de r√¥les permettent aux clusters AKS de tirer les images sans g√©rer manuellement les credentials.

---

## Routage global avec Azure Front Door

### Architecture g√©n√©rale du routage

Azure Front Door constitue la couche sup√©rieure d'une architecture de routage global distribu√©e.[4] Positionn√©e devant les backends distribu√©s g√©ographiquement, cette solution fournit un routage de couche sept sophistiqu√©, une mise en cache distribu√©e et une gestion automatique du basculement.

### Configuration de Base d'Azure Front Door

#### Cr√©ation du profil Front Door

```hcl
resource "azurerm_cdn_frontdoor_profile" "main" {
  name                = "${var.app_name}-frontdoor"
  resource_group_name = azurerm_resource_group.primary.name
  sku_name            = "Standard_AzureFrontDoor"

  tags = {
    environment = var.environment
  }
}
```

Le profil Front Door constitue le conteneur principal pour toute la configuration de routage. La SKU Standard fournit les capacit√©s essentielles pour une architecture haute disponibilit√© moderne.

#### Configuration des origines (origins)

```hcl
# Origine du cluster primaire
resource "azurerm_cdn_frontdoor_origin_group" "primary" {
  name                     = "primary-origin-group"
  cdn_frontdoor_profile_id = azurerm_cdn_frontdoor_profile.main.id
  session_affinity_enabled = true

  load_balancing {
    sample_size                 = 4
    successful_samples_required = 3
    additional_latency_in_milliseconds = 0
  }

  health_probe {
    interval_in_seconds = 100
    path                = "/health"
    protocol            = "Https"
    request_type        = "HEAD"
  }
}

resource "azurerm_cdn_frontdoor_origin" "primary_origin" {
  name                           = "primary-origin"
  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.primary.id
  enabled                        = true
  http_port                      = 80
  https_port                     = 443
  origin_host_header             = azurerm_kubernetes_cluster.primary.fqdn
  priority                       = 1
  weight                         = 50
  certificate_name_check_enabled = true
  host_name                      = azurerm_kubernetes_cluster.primary.fqdn
}

# Origine du cluster secondaire
resource "azurerm_cdn_frontdoor_origin_group" "secondary" {
  name                     = "secondary-origin-group"
  cdn_frontdoor_profile_id = azurerm_cdn_frontdoor_profile.main.id
  session_affinity_enabled = true

  load_balancing {
    sample_size                 = 4
    successful_samples_required = 3
    additional_latency_in_milliseconds = 0
  }

  health_probe {
    interval_in_seconds = 100
    path                = "/health"
    protocol            = "Https"
    request_type        = "HEAD"
  }
}

resource "azurerm_cdn_frontdoor_origin" "secondary_origin" {
  name                           = "secondary-origin"
  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.secondary.id
  enabled                        = true
  http_port                      = 80
  https_port                     = 443
  origin_host_header             = azurerm_kubernetes_cluster.secondary.fqdn
  priority                       = 1
  weight                         = 50
  certificate_name_check_enabled = true
  host_name                      = azurerm_kubernetes_cluster.secondary.fqdn
}
```

Chaque groupe d'origine contient un health probe qui v√©rifie la disponibilit√© des backends toutes les 100 secondes en effectuant une requ√™te HEAD sur l'endpoint `/health`. Si trois requ√™tes cons√©cutives √©chouent, l'origine est marqu√©e comme indisponible. L'affinit√© de session assure que les requ√™tes d'un m√™me client restent dirig√©es vers le m√™me backend lorsque possible.[1]

La pond√©ration (weight) et priorit√© (priority) d√©finissent la distribution du trafic. Avec des poids √©gaux, le trafic se r√©partit √©quitablement. En cas de d√©faillance d'une r√©gion, tout le trafic bascule vers l'autre.

### M√©thodes de routage

#### Routage pond√©r√© (Weighted routing)

```hcl
resource "azurerm_cdn_frontdoor_route" "weighted_route" {
  name                          = "weighted-routing-rule"
  cdn_frontdoor_endpoint_id    = azurerm_cdn_frontdoor_endpoint.main.id
  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.primary.id
  supported_protocols           = ["Http", "Https"]
  patterns_to_match             = ["/*"]
  forwarding_protocol           = "HttpsOnly"
  link_to_default_domain        = true
  https_redirect_enabled        = true

  # Configuration du cache pour les ressources statiques
  cache {
    query_string_caching_behavior = "UseQueryString"
    compression_enabled           = true
  }
}
```

Cette configuration dirige le trafic vers les backends en utilisant le sch√©ma de pond√©ration d√©fini dans les groupes d'origine. Le routage se fait selon la latence mesur√©e et la disponibilit√© de chaque backend.

#### Routage bas√© sur le chemin (Path-based routing)

```hcl
resource "azurerm_cdn_frontdoor_route" "api_route" {
  name                          = "api-routing-rule"
  cdn_frontdoor_endpoint_id    = azurerm_cdn_frontdoor_endpoint.main.id
  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.primary.id
  supported_protocols           = ["Https"]
  patterns_to_match             = ["/api/*"]
  forwarding_protocol           = "HttpsOnly"
  link_to_default_domain        = true
  https_redirect_enabled        = true

  cache {
    query_string_caching_behavior = "IgnoreQueryString"
    compression_enabled           = true
    default_ttl                   = 0  # Pas de cache pour les APIs
  }
}

resource "azurerm_cdn_frontdoor_route" "static_route" {
  name                          = "static-routing-rule"
  cdn_frontdoor_endpoint_id    = azurerm_cdn_frontdoor_endpoint.main.id
  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.primary.id
  supported_protocols           = ["Https"]
  patterns_to_match             = ["/static/*", "*.js", "*.css", "*.jpg", "*.png", "*.gif"]
  forwarding_protocol           = "HttpsOnly"
  link_to_default_domain        = true
  https_redirect_enabled        = true

  cache {
    query_string_caching_behavior = "IgnoreQueryString"
    compression_enabled           = true"
    default_ttl                   = 86400  # Cache 24 heures
    max_ttl                       = 604800 # Maximum 7 jours
  }
}
```

Les r√®gles de routage bas√©es sur le chemin permettent de diriger diff√©rents types de requ√™tes vers des configurations optimis√©es. Les requ√™tes API ne se cachent pas (TTL=0) pour assurer la fra√Æcheur des donn√©es, tandis que les ressources statiques se cachent pendant 24 heures par d√©faut.[4]

### Configuration du point de terminaison Front Door

#### Cr√©ation du point de terminaison

```hcl
resource "azurerm_cdn_frontdoor_endpoint" "main" {
  name                     = "${var.app_name}-endpoint"
  cdn_frontdoor_profile_id = azurerm_cdn_frontdoor_profile.main.id
  enabled                  = true

  tags = {
    environment = var.environment
  }
}
```

L'endpoint Front Door est le point d'entr√©e unique o√π les utilisateurs acc√®dent √† l'application. Un FQDN automatiquement g√©n√©r√© de la forme `example.azurefd.net` est attribu√©.

### Gestion du basculement et haute disponibilit√©

#### Configuration automatique du basculement

```hcl
resource "azurerm_cdn_frontdoor_origin_group" "failover_group" {
  name                     = "failover-origin-group"
  cdn_frontdoor_profile_id = azurerm_cdn_frontdoor_profile.main.id
  session_affinity_enabled = false

  load_balancing {
    sample_size                 = 4
    successful_samples_required = 3
    additional_latency_in_milliseconds = 100
  }

  health_probe {
    interval_in_seconds = 100
    path                = "/health"
    protocol            = "Https"
    request_type        = "HEAD"
  }
}

# Configuration de basculement en priorit√©
resource "azurerm_cdn_frontdoor_origin" "primary_for_failover" {
  name                           = "primary-failover"
  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.failover_group.id
  enabled                        = true
  http_port                      = 80
  https_port                     = 443
  origin_host_header             = azurerm_kubernetes_cluster.primary.fqdn
  priority                       = 1  # Priorit√© haute - utilis√© en premier
  weight                         = 100
  certificate_name_check_enabled = true
  host_name                      = azurerm_kubernetes_cluster.primary.fqdn
}

resource "azurerm_cdn_frontdoor_origin" "secondary_for_failover" {
  name                           = "secondary-failover"
  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.failover_group.id
  enabled                        = true
  http_port                      = 80
  https_port                     = 443
  origin_host_header             = azurerm_kubernetes_cluster.secondary.fqdn
  priority                       = 2  # Priorit√© basse - utilis√© en cas d'indisponibilit√©
  weight                         = 100
  certificate_name_check_enabled = true
  host_name                      = azurerm_kubernetes_cluster.secondary.fqdn
}
```

En mode basculement (failover), les requ√™tes se dirigent d'abord vers le backend prioritaire. Seulement si celui-ci devient indisponible, les requ√™tes basculent vers le backend de priorit√© inf√©rieure. Cela contraste avec la distribution active/active o√π le trafic se r√©partit continuellement entre tous les backends sains.

### Mise en cache distribu√©e

#### Strat√©gies de cache optimis√©es

```hcl
resource "azurerm_cdn_frontdoor_route" "cache_optimized" {
  name                          = "cache-optimized-route"
  cdn_frontdoor_endpoint_id    = azurerm_cdn_frontdoor_endpoint.main.id
  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.primary.id
  supported_protocols           = ["Https"]
  patterns_to_match             = ["/*"]
  forwarding_protocol           = "HttpsOnly"
  link_to_default_domain        = true
  https_redirect_enabled        = true

  cache {
    # Configuration avanc√©e du cache
    query_string_caching_behavior = "UseQueryString"
    compression_enabled           = true
    
    # Headers personnalis√©s influen√ßant le cache
    cache_key_query_string_include_nulls = false
    
    # Comportement par d√©faut
    default_ttl = 3600  # 1 heure
    min_ttl     = 60    # 1 minute
    max_ttl     = 604800 # 7 jours
  }
}
```

La strat√©gie de cache utilise les en-t√™tes HTTP standard (Cache-Control, ETag, Last-Modified) pour d√©terminer la dur√©e de vie des objets. Azure Front Door compresse automatiquement les contenus supportant la compression (texte, JSON, CSS, JavaScript) pour r√©duire la consommation de bande passante.[4]

### Configuration des domaines personnalis√©s

#### Ajout d'un domaine personnalis√© avec HTTPS

```hcl
resource "azurerm_cdn_frontdoor_custom_domain" "main" {
  name                     = "custom-domain"
  cdn_frontdoor_profile_id = azurerm_cdn_frontdoor_profile.main.id
  dns_zone_id              = azurerm_dns_zone.main.id
  host_name                = "app.example.com"

  tls {
    certificate_type    = "ManagedCertificate"
    minimum_tls_version = "TLS12"
  }
}

resource "azurerm_cdn_frontdoor_route" "custom_domain_route" {
  name                          = "custom-domain-route"
  cdn_frontdoor_endpoint_id    = azurerm_cdn_frontdoor_endpoint.main.id
  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.primary.id
  supported_protocols           = ["Https"]
  patterns_to_match             = ["/*"]
  forwarding_protocol           = "HttpsOnly"
  custom_domains                = [azurerm_cdn_frontdoor_custom_domain.main.id]
  link_to_default_domain        = false
  https_redirect_enabled        = true

  cache {
    query_string_caching_behavior = "UseQueryString"
    compression_enabled           = true
  }
}
```

L'utilisation de domaines personnalis√©s avec certificats g√©r√©s offre une exp√©rience utilisateur professionnelle. Azure Front Door g√®re automatiquement le renouvellement des certificats TLS.

### R√®gles d'acheminement avanc√©es (Rules Engine)

#### R√©√©criture d'en-t√™tes HTTP

```hcl
resource "azurerm_cdn_frontdoor_route" "with_rules" {
  name                          = "route-with-rules"
  cdn_frontdoor_endpoint_id    = azurerm_cdn_frontdoor_endpoint.main.id
  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.primary.id
  supported_protocols           = ["Https"]
  patterns_to_match             = ["/*"]
  forwarding_protocol           = "HttpsOnly"
  link_to_default_domain        = true

  # R√®gles d'acheminement avanc√©es
  rule {
    name     = "add-security-headers"
    order    = 1
    operator = "Any"

    actions {
      response_header_actions {
        header_action = "Append"
        header_name   = "X-Content-Type-Options"
        value         = "nosniff"
      }
      response_header_actions {
        header_action = "Append"
        header_name   = "X-Frame-Options"
        value         = "DENY"
      }
      response_header_actions {
        header_action = "Append"
        header_name   = "X-XSS-Protection"
        value         = "1; mode=block"
      }
    }
  }

  rule {
    name     = "compression-optimization"
    order    = 2
    operator = "Any"

    actions {
      cache_expiration_actions {
        cache_behavior = "SetIfMissing"
        cache_duration = "1:30:00"
      }
    }
  }
}
```

Le moteur de r√®gles Front Door permet de modifier les en-t√™tes, de rediriger les requ√™tes ou d'appliquer des transformations complexes. Ces r√®gles s'ex√©cutent √† proximit√© de l'utilisateur final pour minimiser la latence.

### D√©ploiement compl√®te de Front Door avec Terraform

#### Script d'application Terraform

```bash
#!/bin/bash

set -e

echo "=== Validation de la configuration Terraform ==="
terraform validate

echo "=== Formatage de la configuration Terraform ==="
terraform fmt -recursive

echo "=== Planification du d√©ploiement ==="
terraform plan -out=tfplan

echo "=== D√©ploiement de l'infrastructure ==="
terraform apply tfplan

echo "=== R√©cup√©ration des informations de sortie ==="
terraform output -json

echo "=== D√©ploiement termin√© avec succ√®s ==="
```

#### Outputs utiles

```hcl
output "primary_cluster_fqdn" {
  value       = azurerm_kubernetes_cluster.primary.fqdn
  description = "FQDN du cluster AKS primaire"
}

output "secondary_cluster_fqdn" {
  value       = azurerm_kubernetes_cluster.secondary.fqdn
  description = "FQDN du cluster AKS secondaire"
}

output "frontdoor_endpoint" {
  value       = azurerm_cdn_frontdoor_endpoint.main.host_name
  description = "Endpoint Azure Front Door"
}

output "frontdoor_id" {
  value       = azurerm_cdn_frontdoor_profile.main.id
  description = "ID du profil Azure Front Door"
}

output "acr_login_server" {
  value       = azurerm_container_registry.acr.login_server
  description = "Serveur de connexion du registre de conteneurs"
}
```

Ces outputs facilitent la r√©cup√©ration des informations essentielles apr√®s le d√©ploiement sans interroger manuellement le portail Azure.

### Monitoring et diagnostics avec Log Analytics

#### Configuration de Log Analytics

```lol
resource "azurerm_log_analytics_workspace" "shared" {
  name                = "${var.app_name}-law-shared"
  location            = azurerm_resource_group.primary.location
  resource_group_name = azurerm_resource_group.primary.name
  sku                 = "PerGB2018"
  retention_in_days   = 30

  tags = {
    environment = var.environment
  }
}

resource "azurerm_log_analytics_workspace" "primary" {
  name                = "${var.app_name}-law-${var.primary_region}"
  location            = azurerm_resource_group.primary.location
  resource_group_name = azurerm_resource_group.primary.name
  sku                 = "PerGB2018"
  retention_in_days   = 30

  tags = {
    environment = var.environment
    region      = var.primary_region
  }
}

resource "azurerm_log_analytics_workspace" "secondary" {
  name                = "${var.app_name}-law-${var.secondary_region}"
  location            = azurerm_resource_group.secondary.location
  resource_group_name = azurerm_resource_group.secondary.name
  sku                 = "PerGB2018"
  retention_in_days   = 30

  tags = {
    environment = var.environment
    region      = var.secondary_region
  }
}
```

Chaque r√©gion dispose de sa propre instance Log Analytics stockant les m√©triques et journaux de diagnostic r√©gionaux. Une instance partag√©e centralise les donn√©es de toutes les r√©gions pour l'analyse globale.[1]

### Tests de basculement et sc√©narios de r√©silience

#### Simulation d'une d√©faillance r√©gionale

```bash
#!/bin/bash

# Script pour simuler une d√©faillance de r√©gion
REGION_TO_TEST="primary"
PRIMARY_CLUSTER="myapp-aks-westeurope"
PRIMARY_RG="myapp-rg-westeurope"

echo "=== Simulation de d√©faillance de ${REGION_TO_TEST} ==="

# D√©sactiver les pods dans la r√©gion primaire
kubectl config use-context "${PRIMARY_CLUSTER}"
kubectl scale deployment voting-app --replicas=0 -n production

echo "Pods arr√™t√©s dans ${PRIMARY_CLUSTER}"
echo "Attendez que Front Door d√©tecte la d√©faillance (jusqu'√† 100 secondes)..."
sleep 120

# V√©rifier que le trafic se dirige vers la r√©gion secondaire
echo "Envoi de requ√™tes de test..."
for i in {1..10}; do
  response=$(curl -s -H "User-Agent: TestClient" https://app.example.azurefd.net/api/metrics | jq .region)
  echo "Requ√™te $i : r√©gion = ${response}"
done

# Red√©marrer les pods
echo "Red√©marrage des pods dans ${PRIMARY_CLUSTER}..."
kubectl scale deployment voting-app --replicas=3 -n production

echo "=== Test de basculement termin√© ==="
```

Ce test v√©rifie que le syst√®me bascule correctement vers la r√©gion secondaire en cas d'indisponibilit√© de la r√©gion primaire. Azure Front Door d√©tecte la d√©faillance via les health probes et redirige le trafic automatiquement.[1]

#### Utilisation d'Azure Chaos Studio

```bash
#!/bin/bash

# D√©ploiement d'une exp√©rience de chaos pour tester la r√©silience
CHAOS_EXPERIMENT_NAME="aks-region-outage"
CLUSTER_RESOURCE_ID="/subscriptions/{subscription-id}/resourceGroups/myapp-rg-westeurope/providers/Microsoft.ContainerService/managedClusters/myapp-aks-westeurope"

echo "=== Configuration d'une exp√©rience de chaos ==="

az rest --method put \
  --uri "https://management.azure.com${CLUSTER_RESOURCE_ID}/providers/Microsoft.Chaos/experiments/${CHAOS_EXPERIMENT_NAME}?api-version=2024-01-01" \
  --body '{
    "location": "westeurope",
    "identity": {
      "type": "SystemAssigned"
    },
    "properties": {
      "steps": [
        {
          "name": "simulate-pod-failure",
          "branches": [
            {
              "name": "kill-pods",
              "actions": [
                {
                  "type": "continuous",
                  "name": "urn:csci:microsoft:kubernetesCluster:podChaos/2.1",
                  "parameters": [
                    {
                      "name": "action",
                      "value": "kill"
                    },
                    {
                      "name": "namespace",
                      "value": "production"
                    }
                  ],
                  "duration": "PT5M"
                }
              ]
            }
          ]
        }
      ]
    }
  }' \
  --output json
```

Azure Chaos Studio permet de cr√©er et d'ex√©cuter des exp√©riences contr√¥l√©es pour tester la r√©silience du syst√®me face √† des d√©faillances simul√©es. Cela offre une validation pratique de la haute disponibilit√©.

### Validation et v√©rification du d√©ploiement

#### Script de test complet

```bash
#!/bin/bash

set -e

FRONTDOOR_ENDPOINT="https://app.example.azurefd.net"

echo "=== Tests de validation de Front Door ==="

# Test 1 : V√©rification de la disponibilit√© g√©n√©rale
echo "Test 1 : Disponibilit√© g√©n√©rale"
status=$(curl -s -o /dev/null -w "%{http_code}" "${FRONTDOOR_ENDPOINT}/")
if [ "$status" = "200" ]; then
    echo "‚úì Endpoint accessible (HTTP $status)"
else
    echo "‚úó Endpoint indisponible (HTTP $status)"
fi

# Test 2 : V√©rification des health probes
echo "Test 2 : Health probes"
health=$(curl -s "${FRONTDOOR_ENDPOINT}/health" | jq .status)
echo "Statut de sant√© : ${health}"

# Test 3 : V√©rification du routage
echo "Test 3 : Routage vers les backends"
for i in {1..5}; do
    region=$(curl -s "${FRONTDOOR_ENDPOINT}/api/metrics" | jq -r .region)
    instance=$(curl -s "${FRONTDOOR_ENDPOINT}/api/metrics" | jq -r .instance_id)
    echo "Requ√™te $i : r√©gion=${region}, instance=${instance}"
done

# Test 4 : Performance et latence
echo "Test 4 : Performance"
time curl -s -o /dev/null "${FRONTDOOR_ENDPOINT}/"

# Test 5 : Compression de r√©ponse
echo "Test 5 : Compression"
compression=$(curl -s -H "Accept-Encoding: gzip" -I "${FRONTDOOR_ENDPOINT}/" | grep -i "content-encoding")
echo "En-t√™te compression : ${compression}"

echo "=== Validation termin√©e ==="
```

Ce script effectue une s√©rie de tests pour valider le fonctionnement correct de Front Door, y compris la disponibilit√©, le routage, et les performances.

---

## R√©sum√© du parcours d'apprentissage

La progression de ce module suit une structure logique progressant du g√©n√©ral au sp√©cifique. La pr√©paration de l'environnement √©tablit les fondations n√©cessaires √† l'infrastructure. La compr√©hension du code de l'application et de son d√©ploiement d√©monstrent comment les applications modernes s'int√®grent dans les √©cosyst√®mes cloud. Enfin, la configuration d√©taill√©e d'Azure Front Door r√©v√®le comment construire une couche de routage sophistiqu√©e et r√©siliente capable de servir des millions d'utilisateurs distribu√©s g√©ographiquement.

Les principes appliqu√©s dans ce projet -- la redondance g√©ographique, l'√©quilibrage de charge intelligent, la surveillance proactive et l'automatisation du basculement -- constituent les √©l√©ments fondamentaux d'une architecture cloud moderne hautement disponible. La ma√Ætrise de ces concepts pr√©pare aux d√©fis r√©els du d√©ploiement d'applications critiques √† l'√©chelle globale.
